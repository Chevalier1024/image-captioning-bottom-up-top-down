<h1> Image captioning using Bottom-up, Top-down Attention</h1>

This is a PyTorch implementation of Image Captioning using Bottom-up, Top-down Attention as implemened here. Training and evaluation is done on the MSCOCO Image captioning challenge dataset. Bottom up features for MSCOCO dataset are extracted using Faster R-CNN object detection model trained on Visual Genome dataset. Pretrained features are downloaded from 


<h3> Results obtained </h3> 

| Model | BLEU - 4 | METEOR | ROUGE - L | CIDEr |
|-----------------------------------------------|
|Bottom-up,Top-down attention with ReLU| 36.1 | 27.2 | 56.3 | 112.4 |



<h3> Data preparation </h3>

Create a folder called 'data'

Create a folder called 'final_dataset'

Download the MSCOCO Training (13GB) http://images.cocodataset.org/zips/train2014.zip and Validation (6GB) http://images.cocodataset.org/zips/val2014.zip images. 

Also download Andrej Karpathy's training, validation, and test splits http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip. This zip file contains the captions.


Unzip all files and place the folders in 'data' folder.



Next, download the bottom up image features from https://imagecaption.blob.core.windows.net/imagecaption/trainval_36.zip.

Unzip the folder and place unzipped folder in bottom-up_features folder.  

Next type this command in a python 2 environment: python tsv.py

This command will create the following files - 

An HDF5 file containing the bottom up image features for train and val splits, 36 per image for each split, in an I, 36, 2048 tensor where I is the number of images in the split.

PKL files that contain training and validation image IDs mapping to index in HDF5 dataset created above.

Move these files to the folder 'final_dataset'.



Next, type this command: python create_input_files.py

This command will create the following files - 

A JSON file for each split containing the order in which to load the bottom up image features so that they are in lockstep with the captions loaded by the dataloader.
A JSON file for each split with a list of N_c * I encoded captions, where N_c is the number of captions sampled per image. These captions are in the same order as the images in the HDF5 file. Therefore, the ith caption will correspond to the i // N_cth image.
A JSON file for each split with a list of N_c * I caption lengths. The ith value is the length of the ith caption, which corresponds to the i // N_cth image.
A JSON file which contains the word_map, the word-to-index dictionary.


Next go to nlg_eval_master folder and type the following two commands:

pip install -e .
nlg-eval --setup

This will install all the files needed for evaluation.


<h3> Training </h3>

To train the bottom-up top down model from scratch, type:
python train.py

The dataset used for learning and evaluation is the MSCOCO Image captioning challenge dataset. It is split into training, validation and test sets using the popular Karpathy splits. This split contains 113,287 training images with five captions each, and 5K images respectively for validation and testing. Teacher forcing is used to aid convergence during training. Teacher forcing is a method of training sequence based tasks on recurrent neural networks by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network. Teacher forcing addresses slow convergence and instability when training recurrent networks that use model output from a prior time step as an input [1].

ReLU gate was used instead of TanH in the attention model.
Weight normalization was found to prevent the model from overfitting and is used liberally for all fully connected layers.Gradients are clipped during training to prevent gradient explosion [22] that is not uncommon with LSTMs. The attention dimensions, word embedding dimension and hidden dimensions of the LSTMs are set to 1024.Dropout is set to 0.5. Batch size is set to 100. 36 pretrained bottom-up feature maps per image are used as input to the Top-down Attention model. The Adamax optimizer is used with a learning rate of 2e-3. Early stopping is employed if the BLEU-4 score of the validation set shows no improvement over 20 epochs.


<h3> Evaluation </h3>

To evaluate the model, type:
python eval.py

Beam search is used to generate captions during evaluation. Beam search iteratively considers the set of the k best sentences up to time t as candidates to generate sentences of size t + 1, and keeps only the resulting best k of them [27]. A beam search of five is used for inference.The metrics reported are ones used most often in relation to image captioning and include BLEU-4, CIDEr, METEOR and ROUGE-L. Official MSCOCO evaluation scripts are used for measuring these scores 3.
  